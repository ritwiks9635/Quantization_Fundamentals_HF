# Quantization_Fundamentals_HF
Quantization for deep learning is the process of approximating a neural network that uses floating-point numbers by a neural network of low bit width numbers. This dramatically reduces both the memory requirement and computational cost of using neural networks.


**Quantization** is a model size reduction technique that converts model weights from high-precision floating-point representation to low-precision floating-point (FP) or integer (INT) representations, such as 16-bit or 8-bit.

# What is the basic concept of quantization?

Quantization is the process of mapping continuous infinite values to a smaller set of discrete finite values. In the context of simulation and embedded computing, it is about approximating real-world values with a digital representation that introduces limits on the precision and range of a value.

![](https://github.com/ritwiks9635/Quantization_Fundamentals_HF/blob/main/Screenshot_20240420-124755_Chrome.jpg)

![](https://github.com/ritwiks9635/Quantization_Fundamentals_HF/blob/main/Screenshot_20240420-130149_Chrome.jpg)

![](https://github.com/ritwiks9635/Quantization_Fundamentals_HF/blob/main/Screenshot_20240420-130222_Chrome.jpg)
